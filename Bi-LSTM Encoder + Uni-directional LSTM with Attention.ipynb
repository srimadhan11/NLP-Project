{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import libraries and packages","metadata":{"id":"LevTeEwpDR93","papermill":{"duration":0.051018,"end_time":"2021-05-06T19:42:17.140797","exception":false,"start_time":"2021-05-06T19:42:17.089779","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Importing the libraries and packages\nimport numpy as np\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nimport random\nimport re\nimport csv\nfrom sklearn.model_selection import train_test_split\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"FAmAs1LxBlKV","papermill":{"duration":3.078212,"end_time":"2021-05-06T19:42:20.257927","exception":false,"start_time":"2021-05-06T19:42:17.179715","status":"completed"},"tags":[],"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"id":"Rl_PhqKKsFJJ","outputId":"b705ab89-5596-4557-fec0-6eeb75426401","papermill":{"duration":0.080167,"end_time":"2021-05-06T19:42:20.417926","exception":false,"start_time":"2021-05-06T19:42:20.337759","status":"completed"},"tags":[],"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder","metadata":{"id":"lB_LgJgINUhM","papermill":{"duration":0.072164,"end_time":"2021-05-06T19:42:20.556732","exception":false,"start_time":"2021-05-06T19:42:20.484568","status":"completed"},"tags":[],"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# tokenization \n!pip install -U nltk\nimport nltk\nimport sys\nnltk.download('punkt')\nfrom nltk.tokenize import sent_tokenize\nimport nltk.data\nimport pickle\nfrom nltk.tokenize import word_tokenize\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom nltk.translate.meteor_score import single_meteor_score","metadata":{"id":"pZcZ3A1_LQvn","outputId":"d16c7d86-b048-4f6f-b102-24d0cb28fa78","papermill":{"duration":9.282673,"end_time":"2021-05-06T19:42:29.903078","exception":false,"start_time":"2021-05-06T19:42:20.620405","status":"completed"},"tags":[],"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (3.2.4)\nCollecting nltk\n  Downloading nltk-3.6.2-py3-none-any.whl (1.5 MB)\n\u001b[K     |████████████████████████████████| 1.5 MB 873 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk) (7.1.2)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk) (1.0.1)\nRequirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from nltk) (2021.3.17)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk) (4.59.0)\nInstalling collected packages: nltk\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.6.2 which is incompatible.\u001b[0m\nSuccessfully installed nltk-3.6.2\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Prepare Data for the Model","metadata":{"id":"bwVxK_b3kSNe","papermill":{"duration":0.043876,"end_time":"2021-05-06T19:42:29.990087","exception":false,"start_time":"2021-05-06T19:42:29.946211","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Load dataset\nAdd the data from train.csv into **data** as list of lists. Each element of this list **data** is again a list of sentences, the first sentence is the *source sentence*,  and the second sentence is the corresponding *target sentence*.","metadata":{"id":"2Bw1m62tDhbj","papermill":{"duration":0.042745,"end_time":"2021-05-06T19:42:30.07621","exception":false,"start_time":"2021-05-06T19:42:30.033465","status":"completed"},"tags":[]}},{"cell_type":"code","source":"labels = ['LABEL1', 'LABEL2']\nonehot = OneHotEncoder(categories=[labels])\nonehot.fit(np.array(labels).reshape(-1, 1))","metadata":{"id":"0AaCdim7ND0Z","outputId":"25a96bfb-9d93-4f41-8711-f89b5b7ed52f","papermill":{"duration":0.094261,"end_time":"2021-05-06T19:42:30.236219","exception":false,"start_time":"2021-05-06T19:42:30.141958","status":"completed"},"tags":[],"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"OneHotEncoder(categories=[['LABEL1', 'LABEL2']])"},"metadata":{}}]},{"cell_type":"code","source":"dataset = pd.read_csv(\"../input/proj-dataset/en-fr_es_new.csv\", index_col=0).reset_index(drop=True)\n#train_data, test_data = train_test_split(dataset, test_size=0.2, shuffle=False)\ntrain_data = dataset.copy()\ntest_data = dataset[-5000:]","metadata":{"id":"jGb1Vnq9Z-Aj","papermill":{"duration":0.721631,"end_time":"2021-05-06T19:42:31.028378","exception":false,"start_time":"2021-05-06T19:42:30.306747","status":"completed"},"tags":[],"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# save\ntrain_data.to_csv(\"train_p2.csv\", index=False)\ntest_data.to_csv(\"test_p2.csv\", index=False)","metadata":{"id":"mibxUfEcapou","papermill":{"duration":0.767533,"end_time":"2021-05-06T19:42:31.843822","exception":false,"start_time":"2021-05-06T19:42:31.076289","status":"completed"},"tags":[],"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# load\ntrain_data = pd.read_csv(\"train_p2.csv\", index_col=False)\ntest_data = pd.read_csv(\"test_p2.csv\", index_col=False)\n\n","metadata":{"id":"uK-Czi1MazT0","papermill":{"duration":0.221768,"end_time":"2021-05-06T19:42:32.110331","exception":false,"start_time":"2021-05-06T19:42:31.888563","status":"completed"},"tags":[],"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Pre-processing","metadata":{"id":"WIuHgfvsb8Z-","papermill":{"duration":0.043614,"end_time":"2021-05-06T19:42:32.199224","exception":false,"start_time":"2021-05-06T19:42:32.15561","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# performing tokenization on the sentences\n\ndef lang_tokenizer(label, sentence):\n  language = \"\"\n  if label == \"ENGLISH\":\n     language = 'english'\n  elif label =='FRENCH':\n     language = 'french'\n  elif label == 'SPANISH':\n     language = 'spanish'\n  elif label == \"GREEK\":\n     language = 'greek'\n  regex = re.compile(r'[@_!♫♪#$%^&*(.,)<>?/\\|}{~:;-]')\n  sentence = regex.sub('',sentence)                  \n  sentence = sentence.lower()   \n  token_list = word_tokenize(sentence,language=language)                                  #tokenization\n  return token_list ","metadata":{"id":"OuRzcpcna1Cg","papermill":{"duration":0.052938,"end_time":"2021-05-06T19:42:32.295925","exception":false,"start_time":"2021-05-06T19:42:32.242987","status":"completed"},"tags":[],"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# vocab class for the word to index and index to word mapping\n\nclass Vocab:\n   def __init__(self):\n     self.word2index = {\"<unk>\":0 , \"<sos>\":1, \"<eos>\":2, \"<pad>\":3}\n     self.index2word = {0:\"<unk>\" , 1:\"<sos>\", 2:\"<eos>\", 3:\"<pad>\"}\n     self.vocab_size = len(self.word2index)\n     self.word_count = {\"<unk>\":1 , \"<sos>\":1, \"<eos>\":1, \"<pad>\":1}\n\n   def add_to_vocab(self,token_list):\n     for token in token_list:\n        if token not in self.word2index:        #add to vocab only if its not already present \n           ind = len(self.word2index)\n           self.word2index[token] = ind\n           self.index2word[ind] = token\n           self.vocab_size += 1\n           self.word_count[token] = 1\n        else:\n           self.word_count[token] += 1  \n           \n   def tokens2tensor(self,token_list):\n      token_indices = list()\n      \n      #for each token, append its index as per the built vocabulary. If token is not present in the vocab, append the index of <unk> \n      for token in token_list:\n          if token in self.word2index:\n              token_indices.append(self.word2index[token])\n          else:\n              token_indices.append(self.word2index[\"<unk>\"])\n\n      #convert the token_indices into tensor              \n      sentence_tensor = torch.tensor(token_indices).unsqueeze(1).to(device)\n      \n      return sentence_tensor","metadata":{"id":"SjhiHTAvbAzf","papermill":{"duration":0.056168,"end_time":"2021-05-06T19:42:32.39662","exception":false,"start_time":"2021-05-06T19:42:32.340452","status":"completed"},"tags":[],"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"token_columns = {\"src_token_indices\":[], \"label_token_indices\":[],\"tgt_token_indices\":[]}\n\nsrc_field = Vocab()\ntgt_field = Vocab()\nfor i in range(len(train_data)):\n   \n   temp_src = train_data['source'].iloc[i]\n   temp_tgt = train_data['target'].iloc[i]\n   temp_label = train_data['labels'].iloc[i]\n\n   #tokenisation\n   src_tokens = lang_tokenizer(temp_label, temp_src)\n   tgt_tokens = lang_tokenizer(temp_label, temp_tgt)\n\n   #add to vocab\n   src_field.add_to_vocab(src_tokens)\n   tgt_field.add_to_vocab(tgt_tokens)\n\n   #convert tokens to indices and add <sos> at the beginning ang <eos> at the end\n   src_indices = [src_field.word2index['<sos>']] + [src_field.word2index[token] if token in src_field.word2index else src_field.word2index[\"<unk>\"] for token in src_tokens] + [src_field.word2index['<eos>']]\n   tgt_indices = [tgt_field.word2index['<sos>']] + [tgt_field.word2index[token] if token in tgt_field.word2index else tgt_field.word2index[\"<unk>\"] for token in tgt_tokens] + [tgt_field.word2index['<eos>']]\n   \n   #add to dictionary\n   token_columns[\"src_token_indices\"].append(src_indices)\n   token_columns[\"tgt_token_indices\"].append(tgt_indices)\n\n\ntrain_data[\"src_token_indices\"] =  token_columns[\"src_token_indices\"]\ntrain_data[\"tgt_token_indices\"] =  token_columns[\"tgt_token_indices\"]\n\n#calculate the no of tokens in src\ntrain_data[\"len_of_src\"] = train_data[\"src_token_indices\"].apply(len)\ntrain_data[\"len_of_tgt\"] = train_data[\"tgt_token_indices\"].apply(len)\ntrain_data","metadata":{"id":"Eyg3yJWGcE8X","outputId":"bb5bd50c-7af7-4a2c-e62b-1e247833321b","papermill":{"duration":30.499318,"end_time":"2021-05-06T19:43:02.939769","exception":false,"start_time":"2021-05-06T19:42:32.440451","status":"completed"},"tags":[],"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                                                  source  \\\n0      after the introduction of the notes and coins ...   \n1      this reform project needs, however, to be cons...   \n2      however, in order for venezuela to be a full m...   \n3         it is high time that these promises were kept.   \n4      i would like to go through one or two detailed...   \n...                                                  ...   \n59995         the second stage agreement is not perfect.   \n59996  here we are contributing to adding value to th...   \n59997  the commissioner’s statement suggests this cou...   \n59998       they must serve justice and the rule of law.   \n59999               this is precisely what must be done.   \n\n                                                  target   labels  \\\n0      después de la introducción, el 1 de enero de 2...  SPANISH   \n1      ahora bien, hay que abordar con prudencia este...  SPANISH   \n2      cependant, afin que le venezuela soit un membr...   FRENCH   \n3      il est grand temps qu'ils tiennent leurs prome...   FRENCH   \n4      je voudrais aborder une ou deux questions préc...   FRENCH   \n...                                                  ...      ...   \n59995      l'accord de deuxième phase n'est pas parfait.   FRENCH   \n59996  nous contribuons ici à ajouter de la valeur au...   FRENCH   \n59997  la déclaration du commissaire montre que cela ...   FRENCH   \n59998  deben apegarse a la justicia y al estado de de...  SPANISH   \n59999          esto es precisamente lo que debe hacerse.  SPANISH   \n\n                                       src_token_indices  \\\n0      [1, 4, 5, 6, 7, 5, 8, 9, 10, 11, 12, 13, 14, 1...   \n1         [1, 21, 22, 23, 24, 25, 26, 19, 27, 28, 29, 2]   \n2      [1, 25, 30, 31, 32, 33, 26, 19, 34, 35, 36, 37...   \n3             [1, 43, 44, 45, 46, 37, 47, 48, 49, 50, 2]   \n4      [1, 51, 52, 53, 26, 54, 55, 56, 57, 58, 59, 60...   \n...                                                  ...   \n59995          [1, 5, 440, 1873, 1295, 44, 127, 3809, 2]   \n59996  [1, 402, 120, 74, 11293, 26, 12998, 3049, 26, ...   \n59997  [1, 5, 586, 260, 261, 482, 224, 21, 561, 19, 1...   \n59998     [1, 144, 93, 4774, 2545, 9, 5, 95, 7, 1197, 2]   \n59999             [1, 21, 44, 1772, 107, 93, 19, 314, 2]   \n\n                                       tgt_token_indices  len_of_src  \\\n0      [1, 4, 5, 6, 7, 8, 9, 5, 10, 5, 11, 5, 12, 13,...          21   \n1      [1, 21, 22, 23, 24, 25, 26, 27, 28, 29, 5, 30, 2]          12   \n2      [1, 31, 32, 24, 33, 34, 35, 36, 37, 38, 39, 40...          21   \n3      [1, 49, 50, 51, 52, 53, 54, 55, 56, 57, 41, 58...          11   \n4      [1, 59, 60, 61, 62, 63, 64, 65, 66, 67, 5, 68,...          21   \n...                                                  ...         ...   \n59995         [1, 6575, 5, 549, 1287, 357, 198, 5868, 2]           9   \n59996  [1, 160, 5621, 1982, 38, 3294, 5, 6, 7926, 229...          17   \n59997  [1, 6, 608, 217, 1217, 211, 24, 413, 1225, 44,...          12   \n59998  [1, 1598, 44788, 93, 6, 9308, 13, 501, 4507, 5...          11   \n59999         [1, 562, 107, 2414, 121, 24, 583, 1264, 2]           9   \n\n       len_of_tgt  \n0              23  \n1              13  \n2              21  \n3              13  \n4              19  \n...           ...  \n59995           9  \n59996          18  \n59997          12  \n59998          12  \n59999           9  \n\n[60000 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source</th>\n      <th>target</th>\n      <th>labels</th>\n      <th>src_token_indices</th>\n      <th>tgt_token_indices</th>\n      <th>len_of_src</th>\n      <th>len_of_tgt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>after the introduction of the notes and coins ...</td>\n      <td>después de la introducción, el 1 de enero de 2...</td>\n      <td>SPANISH</td>\n      <td>[1, 4, 5, 6, 7, 5, 8, 9, 10, 11, 12, 13, 14, 1...</td>\n      <td>[1, 4, 5, 6, 7, 8, 9, 5, 10, 5, 11, 5, 12, 13,...</td>\n      <td>21</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>this reform project needs, however, to be cons...</td>\n      <td>ahora bien, hay que abordar con prudencia este...</td>\n      <td>SPANISH</td>\n      <td>[1, 21, 22, 23, 24, 25, 26, 19, 27, 28, 29, 2]</td>\n      <td>[1, 21, 22, 23, 24, 25, 26, 27, 28, 29, 5, 30, 2]</td>\n      <td>12</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>however, in order for venezuela to be a full m...</td>\n      <td>cependant, afin que le venezuela soit un membr...</td>\n      <td>FRENCH</td>\n      <td>[1, 25, 30, 31, 32, 33, 26, 19, 34, 35, 36, 37...</td>\n      <td>[1, 31, 32, 24, 33, 34, 35, 36, 37, 38, 39, 40...</td>\n      <td>21</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>it is high time that these promises were kept.</td>\n      <td>il est grand temps qu'ils tiennent leurs prome...</td>\n      <td>FRENCH</td>\n      <td>[1, 43, 44, 45, 46, 37, 47, 48, 49, 50, 2]</td>\n      <td>[1, 49, 50, 51, 52, 53, 54, 55, 56, 57, 41, 58...</td>\n      <td>11</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>i would like to go through one or two detailed...</td>\n      <td>je voudrais aborder une ou deux questions préc...</td>\n      <td>FRENCH</td>\n      <td>[1, 51, 52, 53, 26, 54, 55, 56, 57, 58, 59, 60...</td>\n      <td>[1, 59, 60, 61, 62, 63, 64, 65, 66, 67, 5, 68,...</td>\n      <td>21</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>59995</th>\n      <td>the second stage agreement is not perfect.</td>\n      <td>l'accord de deuxième phase n'est pas parfait.</td>\n      <td>FRENCH</td>\n      <td>[1, 5, 440, 1873, 1295, 44, 127, 3809, 2]</td>\n      <td>[1, 6575, 5, 549, 1287, 357, 198, 5868, 2]</td>\n      <td>9</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>59996</th>\n      <td>here we are contributing to adding value to th...</td>\n      <td>nous contribuons ici à ajouter de la valeur au...</td>\n      <td>FRENCH</td>\n      <td>[1, 402, 120, 74, 11293, 26, 12998, 3049, 26, ...</td>\n      <td>[1, 160, 5621, 1982, 38, 3294, 5, 6, 7926, 229...</td>\n      <td>17</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>59997</th>\n      <td>the commissioner’s statement suggests this cou...</td>\n      <td>la déclaration du commissaire montre que cela ...</td>\n      <td>FRENCH</td>\n      <td>[1, 5, 586, 260, 261, 482, 224, 21, 561, 19, 1...</td>\n      <td>[1, 6, 608, 217, 1217, 211, 24, 413, 1225, 44,...</td>\n      <td>12</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>59998</th>\n      <td>they must serve justice and the rule of law.</td>\n      <td>deben apegarse a la justicia y al estado de de...</td>\n      <td>SPANISH</td>\n      <td>[1, 144, 93, 4774, 2545, 9, 5, 95, 7, 1197, 2]</td>\n      <td>[1, 1598, 44788, 93, 6, 9308, 13, 501, 4507, 5...</td>\n      <td>11</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>59999</th>\n      <td>this is precisely what must be done.</td>\n      <td>esto es precisamente lo que debe hacerse.</td>\n      <td>SPANISH</td>\n      <td>[1, 21, 44, 1772, 107, 93, 19, 314, 2]</td>\n      <td>[1, 562, 107, 2414, 121, 24, 583, 1264, 2]</td>\n      <td>9</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n<p>60000 rows × 7 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#print(sum(train_data[\"labels\"]==\"FRENCH\")) \n#print(sum(train_data[\"labels\"]==\"SPANISH\")) \n\n# EQUAL NUMBER OF TARGET 1 AND TARGET 2 SETENCES\ntrain_data =  train_data.sort_values(by=\"labels\")\ntrain_data = pd.concat([train_data[:25000],train_data[-25000:]])\n\n#print(sum(train_data[\"labels\"]==\"FRENCH\")) \n#print(sum(train_data[\"labels\"]==\"SPANISH\"))","metadata":{"papermill":{"duration":0.284471,"end_time":"2021-05-06T19:43:03.540508","exception":false,"start_time":"2021-05-06T19:43:03.256037","status":"completed"},"tags":[],"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"\nmax_src_len = max(train_data[\"len_of_src\"])\nmax_tgt_len = max(train_data[\"len_of_tgt\"])\n\n# NUMBER OF BATCHES \nbatch_size =8","metadata":{"id":"MZOLjIj_c0jm","papermill":{"duration":0.072937,"end_time":"2021-05-06T19:43:03.691872","exception":false,"start_time":"2021-05-06T19:43:03.618935","status":"completed"},"tags":[],"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# APPLYING PADDING TO MAKE BATCHES OF SIMILAR SIZE\n\ndef add_padding(token_list,length,field):\n    while len(token_list)<length:\n       token_list.append(field.word2index[\"<pad>\"])\n    return token_list","metadata":{"id":"SmU5JyoCcjyu","papermill":{"duration":0.05389,"end_time":"2021-05-06T19:43:03.791548","exception":false,"start_time":"2021-05-06T19:43:03.737658","status":"completed"},"tags":[],"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"batch_list = list()","metadata":{"papermill":{"duration":0.062048,"end_time":"2021-05-06T19:43:03.901334","exception":false,"start_time":"2021-05-06T19:43:03.839286","status":"completed"},"tags":[],"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# CLASSES = ['FRENCH','SPANISH'] (  OR ['FRENCH' , 'GREEK'] depending on the dataset)\n\nclasses = ['FRENCH', 'SPANISH']\nonehot_encoder = OneHotEncoder(categories=[classes])\ntrain_labels = onehot_encoder.fit_transform(np.array(train_data['labels']).reshape(-1, 1)).toarray()","metadata":{"id":"Av6aS39xbAwL","papermill":{"duration":0.073309,"end_time":"2021-05-06T19:43:04.122025","exception":false,"start_time":"2021-05-06T19:43:04.048716","status":"completed"},"tags":[],"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"for i in range(0,len(train_data),batch_size):\n   batch  =  train_data[i:i+batch_size]\n   src_tlen = max(batch[\"len_of_src\"])\n   tgt_tlen = max(batch[\"len_of_tgt\"])\n   train_data[i:i+batch_size][\"src_token_indices\"] = train_data[i:i+batch_size][\"src_token_indices\"].apply(add_padding, args=(src_tlen,src_field)) \n   train_data[i:i+batch_size][\"tgt_token_indices\"] = train_data[i:i+batch_size][\"tgt_token_indices\"].apply(add_padding, args=(tgt_tlen,tgt_field)) \n\n   src = np.vstack(list(train_data[i:i+batch_size][\"src_token_indices\"]))\n   tgt = np.vstack(list(train_data[i:i+batch_size][\"tgt_token_indices\"]))\n\n   batch_list.append([ src, tgt, train_labels[i:i+batch_size]])\n    \nrandom.shuffle(batch_list)","metadata":{"id":"7ksPwE50k-Pl","papermill":{"duration":5.535536,"end_time":"2021-05-06T19:43:09.704293","exception":false,"start_time":"2021-05-06T19:43:04.168757","status":"completed"},"tags":[],"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \"\"\"\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model","metadata":{"id":"Zc6_DjBR2-Bi","papermill":{"duration":0.04812,"end_time":"2021-05-06T19:43:10.124682","exception":false,"start_time":"2021-05-06T19:43:10.076562","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"concatenating the onehot representation of conditional labels with the latent space vectors producedby  the  encoder stack. ","metadata":{}},{"cell_type":"code","source":"'''\nL - number of layers\nN - batch size\nH - hidden size\nC - conditional label size\n'''","metadata":{"id":"UkweN7obJ9L7","outputId":"a0308005-ee47-4eb2-b052-1a92db83c18c","papermill":{"duration":0.05598,"end_time":"2021-05-06T19:43:10.228441","exception":false,"start_time":"2021-05-06T19:43:10.172461","status":"completed"},"tags":[],"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'\\nL - number of layers\\nN - batch size\\nH - hidden size\\nC - conditional label size\\n'"},"metadata":{}}]},{"cell_type":"code","source":"# a : L * N * H\n# b : N * C\n# return : L * N * (H + C)\n\ndef latent_space_concat(a, b):\n    return torch.cat((a, b.expand(a.size(0), -1, -1)), dim=-1)","metadata":{"id":"dwQXxCQnENbE","papermill":{"duration":0.055733,"end_time":"2021-05-06T19:43:10.332246","exception":false,"start_time":"2021-05-06T19:43:10.276513","status":"completed"},"tags":[],"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import torch\n# example\nL, N, H, C = 4, 3, 5, 2\n\n# hidden space\na = torch.arange(L * N * H).reshape(L, N, H)\n\n# conditional labels\nb = torch.arange(N * C).reshape(N, C)\n\n# modified hidden space, [L * N * (H + C)]\nc = latent_space_concat(a, b)","metadata":{"id":"lqU4aSydHPtW","papermill":{"duration":0.091412,"end_time":"2021-05-06T19:43:10.47253","exception":false,"start_time":"2021-05-06T19:43:10.381118","status":"completed"},"tags":[],"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"#### Encoder","metadata":{"id":"ba_SRy1wE3Zc","papermill":{"duration":0.047951,"end_time":"2021-05-06T19:43:10.666323","exception":false,"start_time":"2021-05-06T19:43:10.618372","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class Encode(nn.Module):\n  def __init__(self, e_input, e_embedding,\n                           e_hidden_size, num_layers,dropout):\n    super(Encode, self).__init__()\n\n    # Number of layers in the LSTM\n    self.num_layers = num_layers\n\n    # number of features for LSTM to remember\n    self.hidden_size = e_hidden_size\n\n    # No of final nodes to drop in order to avoid overfitting\n    self.dropout = nn.Dropout(0.5)\n\n    # applying a embedding on the given input_size i.e. the hindi vocabulary \n    self.embedding = nn.Embedding(e_input, e_embedding , padding_idx=3)\n    \n    #Applies a multi-layer Bi-LSTM to an input sequence.\n\n    self.LSTM = nn.LSTM(e_embedding,e_hidden_size,num_layers, dropout = 0.5 , bidirectional=True)\n\n    for weight_name, values in self.LSTM.named_parameters():\n        if 'weight_ih' in weight_name:\n            nn.init.kaiming_normal_(values)\n        elif 'weight_hh' in weight_name:\n            nn.init.orthogonal_(values)\n    weight_param_list = self.LSTM._all_weights\n\n    # Setting the value of the forget gate bias =1 , so as to enable it to learn more by default\n    for ind_list in weight_param_list:\n      for wt in ind_list:\n        if \"bias\" in wt:\n          bias = getattr(self.LSTM,wt)\n          bias.data[:]=0\n          bias.data[(bias.shape[0])//4:bias.shape[0]//2]=1\n        \n    self.linear =  nn.Linear(2*e_hidden_size , e_hidden_size)\n    # Displays the weights and biases , choose the bias->0 as tried putting 1/4 to 1/2 of bias length=1\n    # in order to LSTM to remember more by default but that did not improve the results on the validation set.\n    #print(self.LSTM.bias)\n    #print(self.LSTM.bias_ih_l0.data[:].size()) # printed 4096 and hence set the values of 1/4 -> 1/2 i.e. forget gate value to 1\n    #print(self.LSTM.__dict__.keys())    \n  # Dimensions of input [hindi_sentence_length, batch_size](Every sentence of the batch will have the same size since padding was used to make them of equal size)\n  def forward(self, input):\n    \n    # Dimensions of embedding1[hindi_sentence_length , batch_size , embedding_size]\n    # Here , the words of the sentence x for all the sentences of the batch size \n    # will be learning an embedding which was initially created using the constructor and function above\n    # i.e. nn.Embedding \n    embedding1 = self.embedding(input)\n                \n    # Avoid overfitting by dropping some of the nodes\n    embedding = self.dropout(embedding1)\n    \n    #print(\"input->\",input)\n    \n    #print(\"embedding->\",embedding)\n    # The embedding is then passed through the LSTM layer\n    #Dimensions of output [hindi_sentence_length , batch_size , hidden_size*2]\n    #Dimensions of hidden state [num_layers*2, batch_size, hidden_size]\n    encoder_out , (hidden_state , cell_state) = self.LSTM(embedding)\n    \n    hidden_final =torch.zeros(2,input.shape[1],hidden_state.shape[2]*2).to(device)\n\n    # THIS IS DONE SINCE WHAT WE GET AS OUTPUT FROM A BI-LSTM IS SOMETHING LIKE THIS ::\n    # [FORWARD1,BACKWARD1,FORWARD2,BACKWARD2............]\n    # SO  CONCATENATED THE OUTPUTS OF THE FORWARD AND BACKWARD OF EACH OF THE 2 CONSECUTIVE HIDDEN LAYERS \n    # AND THEN FED THE RESULTANT AS THE HIDDEN LAYER OUTPUT OF THE ENCODER TO THE DECODER.\n    for i in range(0,(hidden_final.shape[0])):\n      hidden_final[i]=torch.cat((hidden_state[i*2,:,:],hidden_state[i*2+1,:,:]),dim=1)\n\n    cell_final =torch.zeros(2,input.shape[1],hidden_state.shape[2]*2).to(device)\n\n    for i in range(0,(cell_final.shape[0])):\n      cell_final[i]=torch.cat((cell_state[i*2,:,:], cell_state[i*2+1,:,:]),dim=1)\n    \n    hidden_final = self.linear(hidden_final)\n    cell_final =  self.linear(cell_final)\n    \n    #print(\"hidden state2-> \",hidden_final)\n    #print(\"cell state2 ->\" , cell_final)\n    return encoder_out , (hidden_final , cell_final)\n","metadata":{"id":"J3J0LzVJ3CHB","papermill":{"duration":0.065595,"end_time":"2021-05-06T19:43:10.781104","exception":false,"start_time":"2021-05-06T19:43:10.715509","status":"completed"},"tags":[],"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# Attention","metadata":{"papermill":{"duration":0.050288,"end_time":"2021-05-06T19:43:10.880696","exception":false,"start_time":"2021-05-06T19:43:10.830408","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class Attention(nn.Module):\n  def __init__(self,e_hidden_size,d_hidden_size):\n\n    super(Attention, self).__init__()\n    self.e_hid = e_hidden_size\n    self.d_hid = d_hidden_size\n    self.attention  =  nn.Linear((e_hidden_size) + d_hidden_size , d_hidden_size)\n    self.del_d_hidden = nn.Linear(d_hidden_size, 1)\n\n  def forward(self,hidden,e_out,batch_size,hsentence_length):\n\n    # We need to repeat the hidden layer of decoder \"hsentence_length\" number of times\n    # This is because the e_out have the dimension of [hsentence_length,batch_size,hidden_dimension]\n    # and since when we are trying to find attention we are searching for all words of hsentence_length and \n    # the result we want after applying the operations, i.e. the final attention vector to be of size\n    # hsentence_length and hence , we need to repeat the hidden layer of decoder \"hsentence_length\"\n    # no of times.\n    # Initially-- #hidden = [num_of_layers,batch_size,d_hidden_size]\n        #e_out = [hsentence_length, batch_size, e_hidden_size]\n   \n    hidden = hidden[:,None] \n\n    hidden = torch.repeat_interleave(hidden,repeats=hsentence_length,dim=1)\n        \n    #print(\"here1 --\", hidden.size() , hidden.squeeze(dim=0).size())\n\n    hidden = hidden.permute(0,1,2)\n\n    e_out = e_out.permute(1, 0, 2)\n    \n    # Now :: #hidden = [batch_size,hsentence_length,d_hidden_size]\n        #e_out = [batch_size,hsentence_length , e_hidden_size ]\n\n    # Now, applying the attention to the hidden of decoder and e_out by first concatenating them in the 3rd\n    # dimension and then applying an activation to it.\n    #print(\"hidden2->\", hidden.size())\n    #print(\"eout2->\",e_out.size())\n\n    concatenated_tensor = torch.cat((hidden, e_out), dim = 2)\n  \n    #print(concatenated_tensor.shape , hidden.shape , e_out.shape)\n    \n    #print(\"h-->\", self.e_hid , self.d_hid)\n    concatenated_tensor  = self.attention(concatenated_tensor)\n    \n    # applying the tanh activation over the concatenated_tensor\n    attn_tensor = torch.tanh(concatenated_tensor)\n\n    #attn_tensor = [batch_size,hsentence_length, d_hidden_size]\n    \n    attention = self.del_d_hidden(attn_tensor) \n    # [batch size, hsentence_length , 1] \n    \n    attention = attention.squeeze(dim=2)\n    #attention= [batch size,hsentence_length]\n\n    # application of softmax activation to the attention vector in order to calculate the attention scores obtained\n    attention_resultant = F.softmax(attention, dim=1)\n    \n\n    # This will give a attention vector wherein for every statement present in the batch \n    # the attention vector corresponding to it will contain what is the importance of each word\n    # of the sentence for the prediction of this new word so the softmax will convert the value corresponding\n    # to each word of the sentence in b/w 0 and 1 , such that the sum of importance of all the words\n    # of a sentence is equal to 1.\n\n    return attention_resultant\n\n\n","metadata":{"papermill":{"duration":0.063609,"end_time":"2021-05-06T19:43:10.993874","exception":false,"start_time":"2021-05-06T19:43:10.930265","status":"completed"},"tags":[],"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"#### Decoder","metadata":{"id":"IMMZytUrFAHr","papermill":{"duration":0.049564,"end_time":"2021-05-06T19:43:11.094542","exception":false,"start_time":"2021-05-06T19:43:11.044978","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class Decode(nn.Module):\n  def __init__(self, d_input, d_embedding,hidden_size,result_size, num_layers,dropout,attn):\n    \n    super(Decode, self).__init__()\n\n\n    # Number of layers in the lstm\n    self.num_layers = num_layers\n    \n    # Output size of the word embedding NN\n    self.embedding_size = d_embedding\n\n    # Set the attention vector \n    self.attn = attn\n\n    # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.\n    self.hidden_size = hidden_size\n\n\n    # The result which is obtained after the LSTM model results are passed through\n    # a fully connected layer to give the final vector which contains a probability\n    # corresponding to each word in the vocabulary.\n    #Set the final result size to be obtained \n    self.result_size = result_size\n    \n    self.output_size = result_size\n    # Regularization parameter\n    self.dropout = nn.Dropout(0.5)\n\n    # Shape (hindi_vocab_size,embedding_size) [input size, embedding dims]\n    #The module that allows you to use Embedding is torch.nn.Embedding.\n    #It takes two parameters : the vocabulary size and the dimensionality of the embedding\n    self.embedding = nn.Embedding(d_input, d_embedding)\n    \n    self.d_embedding = d_embedding\n    \n    # Uni-directional LSTM Decoder\n    self.LSTM = nn.LSTM(d_embedding + 2*(hidden_size-2)+2, hidden_size, num_layers, dropout = 0.5 , bidirectional=False)\n\n    # Weight initialisation using kaiming and orthogonal initialisation\n    for weight_name, values in self.LSTM.named_parameters():\n        if 'weight_ih' in weight_name:\n            nn.init.kaiming_normal_(values)\n        elif 'weight_hh' in weight_name:\n            nn.init.orthogonal_(values)\n    weight_param_list = self.LSTM._all_weights\n\n    # Setting the value of the forget gate bias =1 , so as to enable it to learn more by default\n    for ind_list in weight_param_list:\n      for wt in ind_list:\n        if \"bias\" in wt:\n          bias = getattr(self.LSTM,wt)\n          bias.data[:]=0\n          bias.data[(bias.shape[0])//4:bias.shape[0]//2]=1\n   \n    self.fully_connected = nn.Linear(hidden_size + 2*(hidden_size-2)+2 + d_embedding, result_size)\n\n    self.softmax = nn.Softmax(dim=1)\n  # Shape of x  [batch_size]\n  def forward(self,output_from_prev,en_out,hidden_state,cell_state):\n\n    # Shape of x (1, 32) [1, batch_size]\n    output_from_prev = output_from_prev.unsqueeze(0)\n\n    hidden_state1 = hidden_state[-1]\n    # Shape(1, 32, embedding_Size) [1, batch_size, embedding dims]\n    embedding1 = self.dropout(self.embedding(output_from_prev))\n\n    #Applying the activation function to the embedding\n    embedding = F.relu(embedding1)\n    \n    # Passing the encoder outputs (i.e. the hidden states from the encoder) and the decoder's prev layer hidden\n    # unit to the attention function to get the value of the attention vector.\n    # from the attention function we get the value as [batch_size,en_out.shape[0]i.e. the length of the input hindi statement]\n    attention_value = self.attn(hidden_state1,en_out,embedding1.shape[1],en_out.shape[0])\n\n    # Now we need to use this attention to create a weighted vector , which will be having the importance \n    # corresponding to each word \n    \n    # now in order to get the weighted input whose dimension should be appropriate to be fed to the \n    # LSTM layer i.e. it should be of the dimensions ->[1,batch_size,embedding_size+weighted_vector concatenated to it]\n    # so attention value from batch_size,input_hindi_length ----> [batch_size,1,input_hindi_length] and \n     \n    attention_value  = attention_value.unsqueeze(dim=1)\n\n    # en_out -> [batch_size , input_hindi_length ,e_hidden_size]\n\n    en_out  = en_out.permute(1,0,2)\n\n    # then multiplying both of them to give the weighted vector -> [batch_size,1,e_hidden_size]\n    \n    weighted_tensor = torch.matmul(attention_value,en_out)\n\n    # and then concatenating the embedding[1,batch_size,embedding_size] and the weighted vector [batch_size,1,e_hidden_size] (first permute to -> [1,batch_size,e_hidden_size])\n    weighted_tensor = weighted_tensor.permute(1,0,2)\n    \n    #print(weighted_tensor.size())\n    #print(embedding.size())\n    concats = torch.cat((embedding,weighted_tensor),dim=2)\n   \n\n    # and then feeding as input to the LSTM   \n    results, (hidden_state,cell_state) = self.LSTM(concats,(hidden_state,cell_state))\n\n    # Shape of outputs [1, batch_size , hidden_size]\n    # Shape[num_layers, batch_size , hidden_size] (passing encoder's hs, cs - context vectors)\n   # results, hidden_state = self.GRU(embedding, hidden_state)\n\n    #results = results[:, :, :2*self.hidden_size] + results[:, :, 2*self.hidden_size:]\n\n    #Before putting the lstm output into fc layer it has to be flattened out as done above using\n    # nn.Linear\n     #Output Size =  1*Batch_size*target_vocab_size -> as for each word in all the  batches simulatenously\n    # processing , there would be a prob associated with every word present in the vocab and the word with \n    # max prob is choosen as the value to be fed to the next LSTM input to 1st layer (this is done later on\n     # in the training step)\n    \n    # applying a final linear layer \n    english_pred = self.fully_connected(torch.cat((results,weighted_tensor,embedding),dim=2))\n    \n    \n    # Shape --> predictions [batch_size , output_size]\n    \n    english_pred = english_pred.squeeze(0)\n\n    # return the resultant outputs\n    return english_pred,(hidden_state , cell_state)\n\n","metadata":{"id":"LVxMC_fU4BaB","papermill":{"duration":0.069356,"end_time":"2021-05-06T19:43:11.213137","exception":false,"start_time":"2021-05-06T19:43:11.143781","status":"completed"},"tags":[],"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"#### Seq2Seq","metadata":{"id":"afvV1EVTlOHQ","papermill":{"duration":0.049049,"end_time":"2021-05-06T19:43:11.311249","exception":false,"start_time":"2021-05-06T19:43:11.2622","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n    def __init__(self, input_size, output_size, embedding_size, hidden_size, n_layers, n_labels, dropout, device):\n        super().__init__()\n        # create encoder class object\n        self.encoder = Encode(input_size, embedding_size, hidden_size, n_layers, dropout).to(device)\n        # create attention class object\n        self.attention = Attention(hidden_size+n_labels,2*hidden_size+n_labels).to(device)  \n        # create decoder class object and also pass the attention object to it\n        self.decoder = Decode(output_size, embedding_size, hidden_size+n_labels, output_size, n_layers, dropout,self.attention).to(device)\n        self.device = device\n        \n    def forward(self, input_sentence, target_output_sentence, labels, teacher_forcing_ratio = 0.5):\n        input_sen_len = input_sentence.shape[0]\n        batch_size = input_sentence.shape[1]\n        output_sen_len = target_output_sentence.shape[0]\n        output_vocab_size = self.decoder.output_size\n        \n        #tensor to store predicted words by the decoder\n        predicted_word_indexes = torch.zeros(output_sen_len, batch_size, output_vocab_size).to(self.device)\n\n        #pass the input hindi sentence into the encoder \n        e_output, (hidden,cell) = self.encoder(input_sentence)\n\n        # modified hidden , cell and the output latent space\n        hidden = latent_space_concat(hidden, labels)\n        e_output =  latent_space_concat(e_output,labels)\n        cell =  latent_space_concat(cell,labels)\n        \n        #first input to the decoder is always the init_token, i.e., <sos> token\n        decoder_input = target_output_sentence[0] \n        \n        # iterating over all the words in the output_sen_len \n        for i in range(1, output_sen_len):\n            #pass the previous word along with the hidden and cell states of encoder into the decoder \n            output, (hidden,cell) = self.decoder(decoder_input,e_output,hidden,cell)\n\n            #append the next predicted word\n            predicted_word_indexes[i] = output\n            \n            # usage of teacher forcing ratio\n            use_teacher_forcing = random.random() < teacher_forcing_ratio\n            \n            # finding the word with maximum score\n            best_word = output.argmax(1)\n            \n            if use_teacher_forcing:\n              decoder_input = target_output_sentence[i]\n            else:\n              decoder_input = best_word\n\n        return predicted_word_indexes","metadata":{"id":"bID4KBeV4l2y","papermill":{"duration":0.062983,"end_time":"2021-05-06T19:43:11.423885","exception":false,"start_time":"2021-05-06T19:43:11.360902","status":"completed"},"tags":[],"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# Training phase","metadata":{"id":"DDamH_ad4C8t","papermill":{"duration":0.048654,"end_time":"2021-05-06T19:43:11.521736","exception":false,"start_time":"2021-05-06T19:43:11.473082","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"HYPER-PARAMETERS\n","metadata":{}},{"cell_type":"code","source":"MAX_LENGTH =400\ninput_size = src_field.vocab_size\noutput_size = tgt_field.vocab_size\n#hyperparameters\nnum_epochs = 20\nlearning_rate = 0.001          \nhidden_size = 256\nembedding_size = 256           #same for both LSTM (encoder and decoder)\ndropout = 0.5\nn_layers = 2\nn_labels = 2","metadata":{"id":"7nrSOYMfBqQ3","papermill":{"duration":0.056844,"end_time":"2021-05-06T19:43:11.628444","exception":false,"start_time":"2021-05-06T19:43:11.5716","status":"completed"},"tags":[],"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"#initialize the object of Seq2Seq class\nmodel = Seq2Seq(input_size, output_size, embedding_size, hidden_size, n_layers, n_labels, dropout, device).to(device)","metadata":{"id":"-vG-Aomql7Sy","outputId":"85f8eff9-ccbe-4a4d-dacb-a354f3338033","papermill":{"duration":6.238578,"end_time":"2021-05-06T19:43:17.916118","exception":false,"start_time":"2021-05-06T19:43:11.67754","status":"completed"},"tags":[],"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"The parameter initialization has been done as per the paper :  https://arxiv.org/pdf/1409.3215.pdf","metadata":{"id":"nuX2k7zkV8jO","papermill":{"duration":0.049774,"end_time":"2021-05-06T19:43:18.015074","exception":false,"start_time":"2021-05-06T19:43:17.9653","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#initialize the parameters\ndef init_weights(model):\n    for name, parameter in model.named_parameters():\n        nn.init.uniform_(parameter.data, -0.08, 0.08)\n        \nmodel.apply(init_weights)","metadata":{"id":"viR1g5G8GhjK","outputId":"710f8bc1-fb18-4759-bb5e-ef84db916212","papermill":{"duration":0.064017,"end_time":"2021-05-06T19:43:18.128951","exception":false,"start_time":"2021-05-06T19:43:18.064934","status":"completed"},"tags":[],"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"Seq2Seq(\n  (encoder): Encode(\n    (dropout): Dropout(p=0.5, inplace=False)\n    (embedding): Embedding(22011, 256, padding_idx=3)\n    (LSTM): LSTM(256, 256, num_layers=2, dropout=0.5, bidirectional=True)\n    (linear): Linear(in_features=512, out_features=256, bias=True)\n  )\n  (attention): Attention(\n    (attention): Linear(in_features=772, out_features=514, bias=True)\n    (del_d_hidden): Linear(in_features=514, out_features=1, bias=True)\n  )\n  (decoder): Decode(\n    (attn): Attention(\n      (attention): Linear(in_features=772, out_features=514, bias=True)\n      (del_d_hidden): Linear(in_features=514, out_features=1, bias=True)\n    )\n    (dropout): Dropout(p=0.5, inplace=False)\n    (embedding): Embedding(44789, 256)\n    (LSTM): LSTM(770, 258, num_layers=2, dropout=0.5)\n    (fully_connected): Linear(in_features=1028, out_features=44789, bias=True)\n    (softmax): Softmax(dim=1)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"def translate_sentence(model, src_sentence, src_field, tgt_field, label, device , max_length=400):\n    #tokenize the hindi sentence\n    src_tokens = lang_tokenizer(\"ENGLISH\",src_sentence)\n\n    #convert it into tensor\n    sentence_tensor = src_field.tokens2tensor(src_tokens).long()  \n    label_arr = torch.as_tensor(onehot_encoder.fit_transform(np.array([label]).reshape(-1, 1)).toarray(),dtype=torch.float32).to(device).long()\n    \n\n    with torch.no_grad():\n        #pass the source sentence into the encoder to get the hidden and cell states\n        e_output,(hidden,cell) = model.encoder(sentence_tensor)\n    \n    \n    # modified hidden space, output space and cell space\n    hidden = latent_space_concat(hidden, label_arr)\n    e_output = latent_space_concat(e_output,label_arr)\n    cell =  latent_space_concat(cell,label_arr)\n\n    predicted_word_indices = [tgt_field.word2index[\"<sos>\"]]              #index of <sos> in english vocab\n    predicted_sentence = \"\"\n\n    #repeat until the len of predicted sentence is less than max_length or the decoder predicts <eos>\n    while len(predicted_word_indices)<max_length and predicted_word_indices[-1]!= tgt_field.word2index[\"<eos>\"]:\n        prev_word = [predicted_word_indices[-1]]             \n        prev_word = torch.tensor(prev_word).to(device)                  #convert into tensor\n        best_word = \"\"\n        with torch.no_grad():\n            '''\n             pass the last predicted word along with the hidden and cell state of the encoder\n             into the decoder to get the next predicted word\n            '''\n\n            output,(hidden,cell) = model.decoder(prev_word,e_output,hidden ,cell)\n\n            _ ,best_word = output.data.topk(1)                          #get the best predicted word index\n          \n        predicted_word_indices.append(best_word.item())                 #append it to the list of predicted word indices\n        predicted_sentence += tgt_field.index2word[best_word.item()]+\" \"  #append the word corresponding to the predicted index\n\n    translated_sentence = predicted_sentence.replace(\"<sos>\",\"\").replace(\"<eos>\",\"\")\n    return translated_sentence\n","metadata":{"id":"Q1nLzZgleJMQ","papermill":{"duration":0.062659,"end_time":"2021-05-06T19:43:18.241002","exception":false,"start_time":"2021-05-06T19:43:18.178343","status":"completed"},"tags":[],"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def train_batch_list(model, batch_list, criterion, encoder_optimizer, decoder_optimizer):\n    for idx,(input_sentence,target_sentence,labels) in enumerate(batch_list):\n         #get hindi and their corresponding english sentences from the batch\n        src = torch.transpose(torch.as_tensor(input_sentence, dtype=torch.int64),0,1).to(device)\n        tgt = torch.transpose(torch.as_tensor(target_sentence, dtype=torch.int64),0,1).to(device)\n        labels = torch.as_tensor(labels, dtype=torch.float32).to(device)\n        \n         # eliminating the accumulating gradients \n        encoder_optimizer.zero_grad()\n        decoder_optimizer.zero_grad()    \n\n        #pass the hindi and their corresponding english sentences into the model to get the predicted sentence     \n        predicted_sentence = model(src, tgt, labels) \n\n        #adjust the shapes\n        predicted_sentence = predicted_sentence[1:].reshape(-1, predicted_sentence.shape[2])\n        tgt = tgt[1:].reshape(-1)\n\n        #calculate loss    \n        loss = criterion(predicted_sentence,tgt)\n        \n        #backpropagate loss\n        loss.backward()\n\n        #clip the gradients\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n        #opitimize the parameters according to the propagated loss\n        encoder_optimizer.step()\n        decoder_optimizer.step()\n    return loss ","metadata":{"id":"dA6WyL76kQrD","papermill":{"duration":0.062164,"end_time":"2021-05-06T19:43:18.353906","exception":false,"start_time":"2021-05-06T19:43:18.291742","status":"completed"},"tags":[],"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# reference sentence taken to test on various epochs\nref_sen = \"How are you?\"","metadata":{"id":"4JxY1KyYeOtf","papermill":{"duration":0.056078,"end_time":"2021-05-06T19:43:18.45912","exception":false,"start_time":"2021-05-06T19:43:18.403042","status":"completed"},"tags":[],"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# train function \ndef train(model, batch_list,num_epochs=num_epochs):\n    total_loss = 0\n\n    #initialize the optimizer and the criterion(Loss function) to be used\n    encoder_optimizer = optim.Adam(model.encoder.parameters(),lr=learning_rate)       #using Adam optimizer for encoder \n    decoder_optimizer = optim.Adam(model.decoder.parameters(),lr=learning_rate)       #using Adam optimizer for decoder\n    criterion = nn.CrossEntropyLoss(ignore_index = tgt_field.word2index[\"<pad>\"])       #using CrossEntropyLoss function\n    \n\n    for epoch in range(num_epochs):\n\n        model.eval()\n        ref_out = translate_sentence(model, ref_sen, src_field, tgt_field, \"FRENCH\", device, max_length=400)\n        print(f\"french :: {ref_out}\")\n        ref_out = translate_sentence(model, ref_sen, src_field, tgt_field,\"SPANISH\",device, max_length=400)\n        print(f\"spanish :: {ref_out}\")\n        model.train()\n\n\n        #calculate loss for epoch\n        loss = train_batch_list(model,batch_list,criterion,encoder_optimizer,decoder_optimizer)\n        total_loss += loss.item()\n\n        #save model\n        torch.save(model.state_dict(),f\"proj_model_{epoch+1}_new.pt\")\n        \n        print(f'\\n\\nEpoch: {epoch+1}/{num_epochs}     Loss: {loss.item():.4f}')  \n\n    print(f\"\\n\\n Total loss ::: {total_loss/len(batch_list):.4f}\")\n","metadata":{"id":"IgH4P1jxGwO6","papermill":{"duration":0.061062,"end_time":"2021-05-06T19:43:18.569806","exception":false,"start_time":"2021-05-06T19:43:18.508744","status":"completed"},"tags":[],"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# function call to the train function\ntrain(model,batch_list,num_epochs=num_epochs)","metadata":{"id":"HpSQxuQmrIy5","outputId":"48cf713e-a2b0-4eea-9ced-83d09b43ffde","papermill":{"duration":28132.298041,"end_time":"2021-05-07T03:32:10.919032","exception":false,"start_time":"2021-05-06T19:43:18.620991","status":"completed"},"tags":[],"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"french :: sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre \nspanish :: interétatique interétatique dépassement sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre sobre \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-bebb46b617cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# function call to the train function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-31-15d952743938>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, batch_list, num_epochs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m#calculate loss for epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-29-dc5bf9b5023d>\u001b[0m in \u001b[0;36mtrain_batch_list\u001b[0;34m(model, batch_list, criterion, encoder_optimizer, decoder_optimizer)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m#pass the hindi and their corresponding english sentences into the model to get the predicted sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mpredicted_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m#adjust the shapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-a605feb3967a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_sentence, target_output_sentence, labels, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_sen_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m#pass the previous word along with the hidden and cell states of encoder into the decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m#append the next predicted word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-23-1ab30c536507>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, output_from_prev, en_out, hidden_state, cell_state)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;31m# unit to the attention function to get the value of the attention vector.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# from the attention function we get the value as [batch_size,en_out.shape[0]i.e. the length of the input hindi statement]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mattention_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_state1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0men_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membedding1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0men_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;31m# Now we need to use this attention to create a weighted vector , which will be having the importance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-c3b5bf0bb1c2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden, e_out, batch_size, hsentence_length)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat_interleave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrepeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhsentence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m#print(\"here1 --\", hidden.size() , hidden.squeeze(dim=0).size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"#save model\ntorch.save(model.state_dict(),\"model_saved.pt\" )","metadata":{"id":"LwqawHHw4Mtx","papermill":{"duration":1.628078,"end_time":"2021-05-07T03:32:12.723758","exception":false,"start_time":"2021-05-07T03:32:11.09568","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()","metadata":{"id":"k9-9dWrO_Xtt","papermill":{"duration":0.068775,"end_time":"2021-05-07T03:32:13.21787","exception":false,"start_time":"2021-05-07T03:32:13.149095","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CALCULATING THE BLEU AND METEOR SCORE ","metadata":{}}]}